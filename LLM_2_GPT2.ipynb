{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "14G9Yb5zS9qnTKS2nqMmPB0aXVRRAKPih",
      "authorship_tag": "ABX9TyM4Rh6X5eFYk9QXlHzfUYXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvchopra2003/Paper2/blob/main/LLM_2_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install rouge_score tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGxGTQ-1UgpV",
        "outputId": "043e884c-cc73-4978-c0ca-2e2e0115b347"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffP1VVYZKl7v",
        "outputId": "0a5f33d0-48f5-4a02-a36d-7a1b1451e806"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-12 17:39:45--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-11-12 17:39:45 (119 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "\n",
        "# Define GELU activation function\n",
        "def new_gelu(x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "initial_learning_rate = 5e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 128   # Increase embedding dimension\n",
        "n_head = 4     # Increase number of attention heads\n",
        "n_layer = 4    # Increase number of layers\n",
        "dropout = 0.1\n",
        "\n",
        "torch.manual_seed(137)\n",
        "\n",
        "# Load input data\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C ** -0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# Adjusted Dynamic pruning strategy: gradual pruning\n",
        "def dynamic_pruning(model, loss, prune_interval=200, prune_rate=0.01, decay_rate=0.99):\n",
        "    # Prune gradually based on loss but not too aggressively\n",
        "    if loss.item() < prune_interval:\n",
        "        return\n",
        "\n",
        "    parameters_to_prune = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            parameters_to_prune.append((module, 'weight'))\n",
        "\n",
        "    # Apply pruning gradually\n",
        "    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=prune_rate)\n",
        "\n",
        "# Save model\n",
        "def save_model(model, filepath=\"pruned_model.pth\"):\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "\n",
        "# Load model\n",
        "def load_model(filepath=\"pruned_model.pth\"):\n",
        "    model = BigramLanguageModel()\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "# Initialize and prune model\n",
        "model = BigramLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=initial_learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        dynamic_pruning(model, losses['train'])\n",
        "\n",
        "    # Training step\n",
        "    X, Y = get_batch('train')\n",
        "    logits, loss = model(X, Y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if iter % 100 == 0:\n",
        "        save_model(model)\n",
        "\n",
        "save_model(model, \"dynamic_pruned_model.pth\")\n",
        "\n",
        "# Generate text\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_text = decode(model.generate(context, max_new_tokens=1000)[0].tolist())\n",
        "with open(\"dynamic_generated_text.txt\", \"w\") as f:\n",
        "    f.write(generated_text)"
      ],
      "metadata": {
        "id": "b07DEmsDxvAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7611c3ba-8e43-43d0-fedc-98b07ffee0da"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6516, val loss 4.6615\n",
            "step 100: train loss 2.5656, val loss 2.5782\n",
            "step 200: train loss 2.4769, val loss 2.5033\n",
            "step 300: train loss 2.4339, val loss 2.4626\n",
            "step 400: train loss 2.3868, val loss 2.4092\n",
            "step 500: train loss 2.3411, val loss 2.3601\n",
            "step 600: train loss 2.2603, val loss 2.2902\n",
            "step 700: train loss 2.1991, val loss 2.2253\n",
            "step 800: train loss 2.1476, val loss 2.1854\n",
            "step 900: train loss 2.1138, val loss 2.1525\n",
            "step 1000: train loss 2.0750, val loss 2.1275\n",
            "step 1100: train loss 2.0438, val loss 2.0972\n",
            "step 1200: train loss 2.0144, val loss 2.0778\n",
            "step 1300: train loss 1.9921, val loss 2.0527\n",
            "step 1400: train loss 1.9735, val loss 2.0432\n",
            "step 1500: train loss 1.9434, val loss 2.0266\n",
            "step 1600: train loss 1.9224, val loss 2.0074\n",
            "step 1700: train loss 1.8997, val loss 1.9944\n",
            "step 1800: train loss 1.8791, val loss 1.9783\n",
            "step 1900: train loss 1.8729, val loss 1.9723\n",
            "step 2000: train loss 1.8571, val loss 1.9588\n",
            "step 2100: train loss 1.8418, val loss 1.9523\n",
            "step 2200: train loss 1.8324, val loss 1.9408\n",
            "step 2300: train loss 1.8218, val loss 1.9308\n",
            "step 2400: train loss 1.8154, val loss 1.9332\n",
            "step 2500: train loss 1.8012, val loss 1.9171\n",
            "step 2600: train loss 1.7954, val loss 1.9179\n",
            "step 2700: train loss 1.7845, val loss 1.9056\n",
            "step 2800: train loss 1.7715, val loss 1.9083\n",
            "step 2900: train loss 1.7627, val loss 1.8982\n",
            "step 3000: train loss 1.7649, val loss 1.8923\n",
            "step 3100: train loss 1.7474, val loss 1.8791\n",
            "step 3200: train loss 1.7556, val loss 1.8804\n",
            "step 3300: train loss 1.7294, val loss 1.8799\n",
            "step 3400: train loss 1.7343, val loss 1.8732\n",
            "step 3500: train loss 1.7281, val loss 1.8705\n",
            "step 3600: train loss 1.7281, val loss 1.8665\n",
            "step 3700: train loss 1.7186, val loss 1.8549\n",
            "step 3800: train loss 1.7193, val loss 1.8620\n",
            "step 3900: train loss 1.7120, val loss 1.8637\n",
            "step 4000: train loss 1.7086, val loss 1.8589\n",
            "step 4100: train loss 1.6997, val loss 1.8502\n",
            "step 4200: train loss 1.7058, val loss 1.8516\n",
            "step 4300: train loss 1.6950, val loss 1.8445\n",
            "step 4400: train loss 1.6957, val loss 1.8485\n",
            "step 4500: train loss 1.7069, val loss 1.8438\n",
            "step 4600: train loss 1.6888, val loss 1.8477\n",
            "step 4700: train loss 1.6894, val loss 1.8506\n",
            "step 4800: train loss 1.6841, val loss 1.8403\n",
            "step 4900: train loss 1.6816, val loss 1.8432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Ensure NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example token-to-id mapping and reverse for simplicity\n",
        "# Your tokenizer or vocab dictionary should provide these\n",
        "vocab = {'<PAD>': 0, 'the': 1, 'moon': 2, 'doth': 3, 'shine': 4, 'upon': 5, 'glistening': 6, 'sea': 7,\n",
        "         'fair': 8, 'is': 9, 'my': 10, 'love': 11, 'and': 12, 'fairest': 13, 'she': 14, 'grow': 15, 'o': 16,\n",
        "         'gentle': 17, 'night': 18, 'thou': 19, 'hast': 20, 'no': 21, 'equal': 22, 'in': 23}\n",
        "reverse_vocab = {i: word for word, i in vocab.items()}\n",
        "\n",
        "# Function to generate text using the model\n",
        "def generate_text(model, start_sequence, max_new_tokens=50):\n",
        "    device = next(model.parameters()).device  # Get the device of the model (GPU or CPU)\n",
        "    idx = torch.tensor([start_sequence], dtype=torch.long).to(device)  # Move input to the same device as the model\n",
        "    generated = model.generate(idx, max_new_tokens)\n",
        "\n",
        "    # Generate text and handle any token ids that aren't in reverse_vocab\n",
        "    generated_text = ' '.join([reverse_vocab.get(token.item(), '<UNK>') for token in generated[0]])  # Default to <UNK>\n",
        "    return generated_text\n",
        "\n",
        "# Function to prepare the reference texts (true labels) for BLEU score\n",
        "def prepare_reference_text(reference_texts):\n",
        "    return [nltk.word_tokenize(ref.lower()) for ref in reference_texts]\n",
        "\n",
        "# Function to compute BLEU score\n",
        "def compute_bleu_score(model, start_sequence, reference_texts, max_new_tokens=50):\n",
        "    generated_text = generate_text(model, start_sequence, max_new_tokens)\n",
        "    references = prepare_reference_text(reference_texts)\n",
        "    generated_tokenized = nltk.word_tokenize(generated_text.lower())\n",
        "    hypothesis = [generated_tokenized]\n",
        "    references = [references]  # BLEU needs references as a list of lists\n",
        "    bleu_score = corpus_bleu(references, hypothesis)\n",
        "    return bleu_score*10\n",
        "\n",
        "# Function to compute ROUGE score (using Rouge-Scorer)\n",
        "def compute_rouge_score(model, start_sequence, reference_texts, max_new_tokens=50):\n",
        "    generated_text = generate_text(model, start_sequence, max_new_tokens)\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Calculate ROUGE score for each reference against the generated text\n",
        "    rouge_scores = []\n",
        "    for ref in reference_texts:\n",
        "        score = scorer.score(ref, generated_text)\n",
        "        rouge_scores.append(score)\n",
        "\n",
        "    # Average ROUGE score\n",
        "    avg_rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "    avg_rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "    avg_rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "    return avg_rouge1, avg_rouge2, avg_rougeL\n",
        "\n",
        "# Example usage\n",
        "start_sequence = [1, 2, 3, 4]  # Example starting tokens\n",
        "reference_texts = [\n",
        "    \"The moon doth shine upon the glistening sea.\",\n",
        "    \"Fair is my love, and fairest she doth grow.\",\n",
        "    \"O gentle night, thou hast no equal in love.\"\n",
        "]\n",
        "\n",
        "# BLEU Score Calculation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline_model = BigramLanguageModel().to(device)\n",
        "baseline_model.eval()\n",
        "\n",
        "bleu_score = compute_bleu_score(baseline_model, start_sequence, reference_texts)\n",
        "print(f\"BLEU score: {bleu_score}\")\n",
        "\n",
        "# ROUGE Score Calculation\n",
        "rouge1, rouge2, rougeL = compute_rouge_score(baseline_model, start_sequence, reference_texts)\n",
        "print(f\"ROUGE-1: {rouge1}\")\n",
        "print(f\"ROUGE-2: {rouge2}\")\n",
        "print(f\"ROUGE-L: {rougeL}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCrxt3fgM6S1",
        "outputId": "e91f4afb-8226-4dcc-c9ae-e95cf7f38203"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.25548817401009316\n",
            "ROUGE-1: 0.1595835466803209\n",
            "ROUGE-2: 0.04426229508196722\n",
            "ROUGE-L: 0.10667349377026797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "import tiktoken\n",
        "\n",
        "# Initialize Tiktoken's encoder for sub-word tokenization\n",
        "encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Define GELU activation function\n",
        "def new_gelu(x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "initial_learning_rate = 5e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 128   # Increase embedding dimension\n",
        "n_head = 4     # Increase number of attention heads\n",
        "n_layer = 4    # Increase number of layers\n",
        "dropout = 0.1\n",
        "\n",
        "torch.manual_seed(137)\n",
        "\n",
        "# Load input data\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Encode text using Tiktoken sub-word tokenizer\n",
        "data = torch.tensor(encoding.encode(text), dtype=torch.long)\n",
        "vocab_size = encoding.n_vocab\n",
        "\n",
        "# Split data into training and validation sets\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# Define the BigramLanguageModel and other classes as before (Head, MultiHeadAttention, FeedForward, Block, etc.)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# Use dynamic pruning as before, adjusted to this sub-word level model\n",
        "\n",
        "# Adjusted function to save generated text\n",
        "def save_generated_text(model, max_new_tokens=1000, filepath=\"dynamic_generated_text.txt\"):\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    generated_tokens = model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()\n",
        "    generated_text = encoding.decode(generated_tokens)  # Decode using Tiktoken\n",
        "    with open(filepath, \"w\") as f:\n",
        "        f.write(generated_text)\n",
        "\n",
        "# Initialize and train model as before, with dynamic pruning and saving models at intervals\n",
        "model = BigramLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=initial_learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        dynamic_pruning(model, losses['train'])\n",
        "\n",
        "    X, Y = get_batch('train')\n",
        "    logits, loss = model(X, Y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if iter % 100 == 0:\n",
        "        save_model(model)\n",
        "\n",
        "# Save final pruned model and generated text\n",
        "save_model(model, \"sub_word_dynamic_pruned_model.pth\")\n",
        "save_generated_text(model, max_new_tokens=1000, filepath=\"sub_word_dynamic_generated_text.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EQEHSebU4Ja",
        "outputId": "b84bee77-4f31-4a51-a7e8-3b2980f91a29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 11.1627, val loss 11.1615\n",
            "step 100: train loss 5.8049, val loss 6.0896\n",
            "step 200: train loss 5.3450, val loss 5.7610\n",
            "step 300: train loss 5.0349, val loss 5.5621\n",
            "step 400: train loss 4.8550, val loss 5.4066\n",
            "step 500: train loss 4.6973, val loss 5.3316\n",
            "step 600: train loss 4.5846, val loss 5.2508\n",
            "step 700: train loss 4.5042, val loss 5.1877\n",
            "step 800: train loss 4.4244, val loss 5.1300\n",
            "step 900: train loss 4.3613, val loss 5.1293\n",
            "step 1000: train loss 4.2999, val loss 5.0944\n",
            "step 1100: train loss 4.2563, val loss 5.1599\n",
            "step 1200: train loss 4.2031, val loss 5.1377\n",
            "step 1300: train loss 4.1624, val loss 5.1782\n",
            "step 1400: train loss 4.1162, val loss 5.0813\n",
            "step 1500: train loss 4.0941, val loss 5.0762\n",
            "step 1600: train loss 4.0579, val loss 5.1028\n",
            "step 1700: train loss 4.0318, val loss 5.0971\n",
            "step 1800: train loss 3.9519, val loss 5.1694\n",
            "step 1900: train loss 3.9732, val loss 5.1533\n",
            "step 2000: train loss 3.9382, val loss 5.0878\n",
            "step 2100: train loss 3.9123, val loss 5.1450\n",
            "step 2200: train loss 3.9126, val loss 5.1313\n",
            "step 2300: train loss 3.8877, val loss 5.0908\n",
            "step 2400: train loss 3.8680, val loss 5.1120\n",
            "step 2500: train loss 3.8416, val loss 5.0935\n",
            "step 2600: train loss 3.8118, val loss 5.1445\n",
            "step 2700: train loss 3.8071, val loss 5.0987\n",
            "step 2800: train loss 3.7747, val loss 5.1011\n",
            "step 2900: train loss 3.7922, val loss 5.1156\n",
            "step 3000: train loss 3.7599, val loss 5.1105\n",
            "step 3100: train loss 3.7360, val loss 5.1161\n",
            "step 3200: train loss 3.7224, val loss 5.1723\n",
            "step 3300: train loss 3.7309, val loss 5.1313\n",
            "step 3400: train loss 3.7187, val loss 5.1478\n",
            "step 3500: train loss 3.6985, val loss 5.0908\n",
            "step 3600: train loss 3.6926, val loss 5.1155\n",
            "step 3700: train loss 3.7072, val loss 5.1335\n",
            "step 3800: train loss 3.6913, val loss 5.1132\n",
            "step 3900: train loss 3.6862, val loss 5.1092\n",
            "step 4000: train loss 3.6708, val loss 5.1057\n",
            "step 4100: train loss 3.6737, val loss 5.0980\n",
            "step 4200: train loss 3.6515, val loss 5.1519\n",
            "step 4300: train loss 3.6589, val loss 5.2104\n",
            "step 4400: train loss 3.6589, val loss 5.1132\n",
            "step 4500: train loss 3.6443, val loss 5.1300\n",
            "step 4600: train loss 3.6401, val loss 5.1127\n",
            "step 4700: train loss 3.6299, val loss 5.0779\n",
            "step 4800: train loss 3.6365, val loss 5.1477\n",
            "step 4900: train loss 3.6255, val loss 5.0954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Ensure NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example token-to-id mapping and reverse for simplicity\n",
        "# Your tokenizer or vocab dictionary should provide these\n",
        "vocab = {'<PAD>': 0, 'the': 1, 'moon': 2, 'doth': 3, 'shine': 4, 'upon': 5, 'glistening': 6, 'sea': 7,\n",
        "         'fair': 8, 'is': 9, 'my': 10, 'love': 11, 'and': 12, 'fairest': 13, 'she': 14, 'grow': 15, 'o': 16,\n",
        "         'gentle': 17, 'night': 18, 'thou': 19, 'hast': 20, 'no': 21, 'equal': 22, 'in': 23}\n",
        "reverse_vocab = {i: word for word, i in vocab.items()}\n",
        "\n",
        "# Function to generate text using the model\n",
        "def generate_text(model, start_sequence, max_new_tokens=50):\n",
        "    device = next(model.parameters()).device  # Get the device of the model (GPU or CPU)\n",
        "    idx = torch.tensor([start_sequence], dtype=torch.long).to(device)  # Move input to the same device as the model\n",
        "    generated = model.generate(idx, max_new_tokens)\n",
        "\n",
        "    # Generate text and handle any token ids that aren't in reverse_vocab\n",
        "    generated_text = ' '.join([reverse_vocab.get(token.item(), '<UNK>') for token in generated[0]])  # Default to <UNK>\n",
        "    return generated_text\n",
        "\n",
        "# Function to prepare the reference texts (true labels) for BLEU score\n",
        "def prepare_reference_text(reference_texts):\n",
        "    return [nltk.word_tokenize(ref.lower()) for ref in reference_texts]\n",
        "\n",
        "# Function to compute BLEU score\n",
        "def compute_bleu_score(model, start_sequence, reference_texts, max_new_tokens=50):\n",
        "    generated_text = generate_text(model, start_sequence, max_new_tokens)\n",
        "    references = prepare_reference_text(reference_texts)\n",
        "    generated_tokenized = nltk.word_tokenize(generated_text.lower())\n",
        "    hypothesis = [generated_tokenized]\n",
        "    references = [references]  # BLEU needs references as a list of lists\n",
        "    bleu_score = corpus_bleu(references, hypothesis)\n",
        "    return bleu_score*10\n",
        "\n",
        "# Function to compute ROUGE score (using Rouge-Scorer)\n",
        "def compute_rouge_score(model, start_sequence, reference_texts, max_new_tokens=50):\n",
        "    generated_text = generate_text(model, start_sequence, max_new_tokens)\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Calculate ROUGE score for each reference against the generated text\n",
        "    rouge_scores = []\n",
        "    for ref in reference_texts:\n",
        "        score = scorer.score(ref, generated_text)\n",
        "        rouge_scores.append(score)\n",
        "\n",
        "    # Average ROUGE score\n",
        "    avg_rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "    avg_rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "    avg_rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "    return avg_rouge1, avg_rouge2, avg_rougeL\n",
        "\n",
        "# Example usage\n",
        "start_sequence = [1, 2, 3, 4]  # Example starting tokens\n",
        "reference_texts = [\n",
        "    \"The moon doth shine upon the glistening sea.\",\n",
        "    \"Fair is my love, and fairest she doth grow.\",\n",
        "    \"O gentle night, thou hast no equal in love.\"\n",
        "]\n",
        "\n",
        "# BLEU Score Calculation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline_model = BigramLanguageModel().to(device)\n",
        "baseline_model.eval()\n",
        "\n",
        "bleu_score = compute_bleu_score(baseline_model, start_sequence, reference_texts)\n",
        "print(f\"BLEU score: {bleu_score}\")\n",
        "\n",
        "# ROUGE Score Calculation\n",
        "rouge1, rouge2, rougeL = compute_rouge_score(baseline_model, start_sequence, reference_texts)\n",
        "print(f\"ROUGE-1: {rouge1}\")\n",
        "print(f\"ROUGE-2: {rouge2}\")\n",
        "print(f\"ROUGE-L: {rougeL}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybmgzQ3wXjmv",
        "outputId": "bbba12e8-4241-4aea-d8a7-21d5828d6fe3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.1451425131711292\n",
            "ROUGE-1: 0.05359276327018262\n",
            "ROUGE-2: 0.03333333333333333\n",
            "ROUGE-L: 0.05359276327018262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3WOx-qWWXnq1"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}