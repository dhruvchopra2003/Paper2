{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMg5Mzm8m3Gtpke4RXHlLOk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvchopra2003/Paper2/blob/main/LLM_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvRdvxvvq7Vt",
        "outputId": "528e59ff-8244-493b-bb3b-874bc7fd5dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-13 11:10:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-10-13 11:10:36 (46.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in\n",
        "block_size = 32 # maximum context length for\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "#_________________\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Here we are trying to create a character level language\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# train test split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "  # Generate small batches of data of inputs x and y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "  # This is being done to push the tensors onto the GPU to accelerate training\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "\n",
        "#This is basically to optimize pytorch. It tells that whatever intermediate\n",
        "# variables are created, don't store them, coz we're never gonna call backwards\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "# The bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads from the logits for the next token from the lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx and targets are both (B, T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the prediction\n",
        "      logits, loss = self(idx)\n",
        "      # Focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # Apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # Applied sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# Create pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  xb, yb = get_batch('train')\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# Generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKreGNe3Ifgh",
        "outputId": "41630dbc-2603-4e2c-8223-d57b0ea4bcc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7260, val loss 4.7259\n",
            "step 100: train loss 4.5986, val loss 4.5953\n",
            "step 200: train loss 4.4763, val loss 4.4755\n",
            "step 300: train loss 4.3572, val loss 4.3574\n",
            "step 400: train loss 4.2446, val loss 4.2459\n",
            "step 500: train loss 4.1332, val loss 4.1303\n",
            "step 600: train loss 4.0279, val loss 4.0317\n",
            "step 700: train loss 3.9268, val loss 3.9321\n",
            "step 800: train loss 3.8344, val loss 3.8387\n",
            "step 900: train loss 3.7425, val loss 3.7476\n",
            "step 1000: train loss 3.6578, val loss 3.6587\n",
            "step 1100: train loss 3.5823, val loss 3.5821\n",
            "step 1200: train loss 3.5049, val loss 3.5055\n",
            "step 1300: train loss 3.4314, val loss 3.4358\n",
            "step 1400: train loss 3.3648, val loss 3.3694\n",
            "step 1500: train loss 3.2985, val loss 3.3034\n",
            "step 1600: train loss 3.2409, val loss 3.2487\n",
            "step 1700: train loss 3.1900, val loss 3.1855\n",
            "step 1800: train loss 3.1357, val loss 3.1408\n",
            "step 1900: train loss 3.0850, val loss 3.0887\n",
            "step 2000: train loss 3.0422, val loss 3.0514\n",
            "step 2100: train loss 3.0007, val loss 3.0041\n",
            "step 2200: train loss 2.9589, val loss 2.9662\n",
            "step 2300: train loss 2.9247, val loss 2.9305\n",
            "step 2400: train loss 2.8890, val loss 2.8972\n",
            "step 2500: train loss 2.8583, val loss 2.8649\n",
            "step 2600: train loss 2.8261, val loss 2.8387\n",
            "step 2700: train loss 2.8059, val loss 2.8068\n",
            "step 2800: train loss 2.7795, val loss 2.7798\n",
            "step 2900: train loss 2.7514, val loss 2.7597\n",
            "step 3000: train loss 2.7310, val loss 2.7361\n",
            "step 3100: train loss 2.7093, val loss 2.7199\n",
            "step 3200: train loss 2.6949, val loss 2.7014\n",
            "step 3300: train loss 2.6800, val loss 2.6906\n",
            "step 3400: train loss 2.6659, val loss 2.6720\n",
            "step 3500: train loss 2.6542, val loss 2.6589\n",
            "step 3600: train loss 2.6422, val loss 2.6397\n",
            "step 3700: train loss 2.6254, val loss 2.6312\n",
            "step 3800: train loss 2.6125, val loss 2.6232\n",
            "step 3900: train loss 2.5985, val loss 2.6159\n",
            "step 4000: train loss 2.5917, val loss 2.5982\n",
            "step 4100: train loss 2.5802, val loss 2.5929\n",
            "step 4200: train loss 2.5774, val loss 2.5835\n",
            "step 4300: train loss 2.5641, val loss 2.5792\n",
            "step 4400: train loss 2.5629, val loss 2.5713\n",
            "step 4500: train loss 2.5629, val loss 2.5665\n",
            "step 4600: train loss 2.5494, val loss 2.5590\n",
            "step 4700: train loss 2.5447, val loss 2.5575\n",
            "step 4800: train loss 2.5393, val loss 2.5499\n",
            "step 4900: train loss 2.5314, val loss 2.5436\n",
            "\n",
            "\n",
            "\n",
            "CExfikRO:\n",
            "wcowindake la, btKEY: sen bobe d e.\n",
            "S:gO:33SA:\n",
            "\n",
            "\n",
            "LTanss:\n",
            "WanthaiNu qur, vet?\n",
            "F dXENDoate awice my.\n",
            "\n",
            "Thstacomzoroup\n",
            "Yow&$FMOUf isth ble mil;KI ll, ath iree sengmin lat Heriliov ts, anend l nghir.\n",
            "Swanousel lind me l.\n",
            "MAull ce hiry:\n",
            "Supr aisspllw y.\n",
            "Jurindu n Boopetelaves\n",
            "MP:\n",
            "\n",
            "Pl, d motSSkllo W-S:\n",
            "FourtCeiib3s the m dourivETENGShire s p-LOK:\n",
            "\n",
            "NxTre\n",
            "\n",
            "ALONomnterupt f s ar iris! m:\n",
            "\n",
            "Thiny aleronth,\n",
            "MadPre?\n",
            "\n",
            "WISo myr f-NLIE!\n",
            "KENob&y, wardsal thesE: fond uin cNI ayaraney Iry ts I&fr t c!\n",
            "MykenEE:\n",
            "PAke mary.\n",
            "Yof 'WWh wne?m sora anghse.-e?nomangqqurten.\n",
            "Wand tho-me cin s th llugivome.\n",
            "I muco ffspy tssthecas l.\n",
            "TAnEias wethal wave.\n",
            "se ed PTABene oveveaimoCas!\n",
            "\n",
            "\n",
            "\n",
            "Cos cok hedin tie s ingo he te feRUCatLI:\n",
            "Wh$g Clo gscP!-nisthou ldu It n,Forxlone.\n",
            "\n",
            "An!\n",
            "HAnkes aghercoERWAws m k s withoumas Fond ths wllo INour id, mFosedInsurd?\n",
            "TIOMy hithin&XnGenond CDWity\n",
            "K:\n",
            "BIUSHou tiunorthornofen e sutan Ciporthare whanothavitthers,l pESBllollke, on s h O, t pan, ce wat d&Live Wout ir f au;\n",
            "yonkzeginee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Logic behind self attention"
      ],
      "metadata": {
        "id": "dfnA8ST8JMpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtqM7sDFMVtA",
        "outputId": "9a4bb930-4ecd-48c2-e610-5f47478a625f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need the tokens to interact with each other. We basically want each token to have some representation in terms of it's preceeding tokens. Simplest way is to take the average of the channels of each predecessor."
      ],
      "metadata": {
        "id": "3iN_9_z7_YzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C)) # bow aka: bag of words -> used for averaging a set of words\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "FQqsCTSQJTx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVpqMP1mK4O-",
        "outputId": "d61327cc-cbd7-45de-cf87-3341613ec3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSdIIgYSBNeC",
        "outputId": "e809fb32-624b-4094-cc31-226c0e03b233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.3596, -0.9152],\n",
              "        [ 0.6258,  0.0255],\n",
              "        [ 0.9545,  0.0643],\n",
              "        [ 0.3612,  1.1679],\n",
              "        [-1.3499, -0.5102],\n",
              "        [ 0.2360, -0.2398],\n",
              "        [-0.9211,  1.5433]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0] # each row is the average of all the elements above it (in the same column)\n",
        "# This implies that the last row has the average of the entire set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHGZhlR0BPRw",
        "outputId": "52300b81-030f-4c0d-febf-cc446c8902a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=False)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkUaXyrVHTsV",
        "outputId": "52b46c80-69f0-4658-cccc-940d0badfcce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000],\n",
              "        [1.0000, 0.5000, 0.0000],\n",
              "        [1.0000, 0.5000, 0.3333]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can be much more efficient doing this by using matrix multiplication\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbmLP-Y5Bk0o",
        "outputId": "1d4af012-d2fa-41ab-8169-b905d0d90f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the above code, after using multiplying tensor b with the row averaged lower triangular matrix, we find that in tensor c, each row is the\n",
        "# average of the rows above it.\n",
        "# This means that c[n-1] row has the average of all the rows before it"
      ],
      "metadata": {
        "id": "409VzFbmIKHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's create using this averaging logic (using a matrix a [lower triangular matrix with row averaged weights]) and apply it to vectorize\n",
        "# our bag of words tensor before\n",
        "# This would help us to associate each word with all it's predecessors\n",
        "\n",
        "# The lower triangular matrix actually ensures that each element only gets information from it's predecessors and not it's successors."
      ],
      "metadata": {
        "id": "ZG1OosMvIwgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei/torch.sum(wei, 1, keepdim=True)\n",
        "print(f\"Weight matrix: \\n{wei}\") # each row of it sums to 1\n",
        "\n",
        "# our b here is: x\n",
        "print(f\"  Input matrix: \\n{x}\")\n",
        "\n",
        "# Notice that there is a slight difference in the dimensions. X has dimensions: (B, T, C) however wei has dimensions (T, T).\n",
        "# Torch manages itself and creates batches (B) for\n",
        "# the wei matrix to be able to multiply correctly\n",
        "\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)\n",
        "torch.allclose(xbow, xbow2, rtol=1e-04, atol=1e-06)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2djxWsYnJMZR",
        "outputId": "f2f360b7-c3d8-4919-83e8-8aac816a6367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight matrix: \n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "  Input matrix: \n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "# torch.allclose(xbow, xbow3)\n",
        "torch.allclose(xbow, xbow3, rtol=1e-04, atol=1e-06)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lnRznMAMQzH",
        "outputId": "fbd2994c-0e66-452f-b184-57b8a670ce17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making some small changes to the script"
      ],
      "metadata": {
        "id": "H2298Y-RPOBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Changes:\n",
        "  -> remove vocab size from the BigramLanguageModel constructor as it is already defined.\n",
        "  -> Introducing a new variable n_embd: short for number of embedding dimensions\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in\n",
        "block_size = 32 # maximum context length for\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "#__________________________________________________________________________\n",
        "\n",
        "torch.manual_seed(1336)\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Here we are trying to create a character level language\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# train test split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "  # Generate small batches of data of inputs x and y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "  # This is being done to push the tensors onto the GPU to accelerate training\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "\n",
        "#This is basically to optimize pytorch. It tells that whatever intermediate\n",
        "# variables are created, don't store them, coz we're never gonna call backwards\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "# _______________________________________________________________________________________\n",
        "\n",
        "# The bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads from the logits for the next token from the lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size) # This layer is used to go from token embeddings to the logits\n",
        "    # LM_head stands for language model head\n",
        "\n",
        "    # Now we create a provision to use positional embeddings along with the embeddings of the token identities (token_embeddings)\n",
        "    # self.position_embedding_table = nn.Embedding(block_size, n_embd) # each position from 0 to block size would also get it's own vector\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "\n",
        "    # idx and targets are both (B, T) tensor of integers\n",
        "    # When we replace the vocab size with n_embd, it wouldn't give us logits directly, but rather token embeddings\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    # pos_embedding = self.position_embedding_table(torch.arange(T, device=device)) # (T, C) | torch.arange gives integers from 0 to T-1\n",
        "    # x = tok_emb + pos_embedding # (B, T, C) TODO: Check working. the lower dimensional vector (T, C) gets right aligned and another col is added to perform the operation\n",
        "    logits = self.lm_head(tok_emb) # (B, T, vocab_size())\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the prediction\n",
        "      logits, loss = self(idx)\n",
        "      # Focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # Apply softmax to get probabilities\n",
        "      # Introduce a small value (epsilon) to add to the logits before softmax\n",
        "      epsilon = 1e-6\n",
        "      probs = F.softmax(logits + epsilon, dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # sample from the distribution\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "      # Applied sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "# _____________________________________________________________________________\n",
        "\n",
        "# Create model\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# Create pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  xb, yb = get_batch('train')\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# Generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dn6ubgLb4pJ",
        "outputId": "014661ed-e81e-469b-fe11-de2bbaf2b46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3797, val loss 4.3634\n",
            "step 100: train loss 3.5072, val loss 3.5054\n",
            "step 200: train loss 3.0373, val loss 3.0381\n",
            "step 300: train loss 2.8189, val loss 2.8195\n",
            "step 400: train loss 2.7122, val loss 2.7211\n",
            "step 500: train loss 2.6485, val loss 2.6498\n",
            "step 600: train loss 2.6063, val loss 2.6238\n",
            "step 700: train loss 2.5839, val loss 2.5861\n",
            "step 800: train loss 2.5690, val loss 2.5772\n",
            "step 900: train loss 2.5456, val loss 2.5653\n",
            "step 1000: train loss 2.5300, val loss 2.5446\n",
            "step 1100: train loss 2.5272, val loss 2.5370\n",
            "step 1200: train loss 2.5156, val loss 2.5309\n",
            "step 1300: train loss 2.5094, val loss 2.5161\n",
            "step 1400: train loss 2.5146, val loss 2.5175\n",
            "step 1500: train loss 2.5019, val loss 2.5209\n",
            "step 1600: train loss 2.5077, val loss 2.5130\n",
            "step 1700: train loss 2.4954, val loss 2.5114\n",
            "step 1800: train loss 2.4934, val loss 2.5107\n",
            "step 1900: train loss 2.4866, val loss 2.5067\n",
            "step 2000: train loss 2.4887, val loss 2.5094\n",
            "step 2100: train loss 2.4894, val loss 2.5096\n",
            "step 2200: train loss 2.4852, val loss 2.5031\n",
            "step 2300: train loss 2.4832, val loss 2.5115\n",
            "step 2400: train loss 2.4860, val loss 2.4938\n",
            "step 2500: train loss 2.4854, val loss 2.5069\n",
            "step 2600: train loss 2.4815, val loss 2.4966\n",
            "step 2700: train loss 2.4758, val loss 2.4959\n",
            "step 2800: train loss 2.4748, val loss 2.4972\n",
            "step 2900: train loss 2.4806, val loss 2.4927\n",
            "step 3000: train loss 2.4771, val loss 2.5036\n",
            "step 3100: train loss 2.4751, val loss 2.4954\n",
            "step 3200: train loss 2.4711, val loss 2.4958\n",
            "step 3300: train loss 2.4732, val loss 2.5009\n",
            "step 3400: train loss 2.4715, val loss 2.4986\n",
            "step 3500: train loss 2.4755, val loss 2.4958\n",
            "step 3600: train loss 2.4687, val loss 2.4988\n",
            "step 3700: train loss 2.4763, val loss 2.4952\n",
            "step 3800: train loss 2.4712, val loss 2.4901\n",
            "step 3900: train loss 2.4648, val loss 2.4957\n",
            "step 4000: train loss 2.4713, val loss 2.4984\n",
            "step 4100: train loss 2.4676, val loss 2.4937\n",
            "step 4200: train loss 2.4727, val loss 2.4968\n",
            "step 4300: train loss 2.4708, val loss 2.4946\n",
            "step 4400: train loss 2.4707, val loss 2.4900\n",
            "step 4500: train loss 2.4670, val loss 2.4970\n",
            "step 4600: train loss 2.4669, val loss 2.4926\n",
            "step 4700: train loss 2.4695, val loss 2.4935\n",
            "step 4800: train loss 2.4654, val loss 2.4918\n",
            "step 4900: train loss 2.4645, val loss 2.4912\n",
            "\n",
            "\n",
            "ADWAn, tore wins oouyoreowe ee, aved tonde freame f Ifomes dsom Ithed ailowe hale;\n",
            "WINoomedr blldait-Foanses\n",
            "Tifree;\n",
            "ARLInd\n",
            "R I'd m, fesetird? onsto l an. l n ct, ay go th bu ar sts d?\n",
            "Mase we, mere t g hy &sthan\n",
            "UCEFincetok lisin, s ENCly the ws nouctosntharo s clis un ofofathayo seres h n.\n",
            "O:\n",
            "WAtyosING.\n",
            "ARWale womat E pod tas.\n",
            "Whorthe s othentoure an, twel d f l r wilou kne IZAROMESaitherd ad heelyoman onthatampitoden pr sto s avimeloforil-mue tha acier'cte alo Pavtimcu me bbe miel; mann s ge mbust to tapilvece ert t, bowe, Jut d, fo ulll betil, s shirill, ard brs; bund andonthaty aithesup:REThaceruatha jues!\n",
            "SS: RLAno JUEeneney grenonorathewiso,\n",
            "Faleean d ucan byoravee ndo man.\n",
            "AFou t, fenerinel, tourithe wispr--jutourarae ouemystheals t thofact?\n",
            "So mosp torderyovencousthis s pody se?\n",
            "Sto the mertoRo'ssinlangheandintond-\n",
            "WHPds? w se l hincow'd tthe er I winorereadounce.\n",
            "I t orghacfely tld se VI R:\n",
            "\n",
            "S:\n",
            "Honsthat tenchey gh at, f me we t scher ade, BUCeroug we bom IZAncl CHaleroowiche\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There is an error while implementing positional encodings"
      ],
      "metadata": {
        "id": "OQdc6UT3cb8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Changes:\n",
        "  -> remove vocab size from the BigramLanguageModel constructor as it is already defined.\n",
        "  -> Introducing a new variable n_embd: short for number of embedding dimensions\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in\n",
        "block_size = 32 # maximum context length for\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "#__________________________________________________________________________\n",
        "\n",
        "torch.manual_seed(137)\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Here we are trying to create a character level language\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# train test split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "  # Generate small batches of data of inputs x and y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "  # This is being done to push the tensors onto the GPU to accelerate training\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "\n",
        "#This is basically to optimize pytorch. It tells that whatever intermediate\n",
        "# variables are created, don't store them, coz we're never gonna call backwards\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "# _______________________________________________________________________________________\n",
        "\n",
        "# The bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads from the logits for the next token from the lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size) # This layer is used to go from token embeddings to the logits\n",
        "    # LM_head stands for language model head\n",
        "\n",
        "    # Now we create a provision to use positional embeddings along with the embeddings of the token identities (token_embeddings)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) # each position from 0 to block size would also get it's own vector\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "\n",
        "    # idx and targets are both (B, T) tensor of integers\n",
        "    # When we replace the vocab size with n_embd, it wouldn't give us logits directly, but rather token embeddings\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_embedding = self.position_embedding_table(torch.arange(T, device=device)) # (T, C) | torch.arange gives integers from 0 to T-1\n",
        "    x = tok_emb + pos_embedding # (B, T, C) TODO: Check working. the lower dimensional vector (T, C) gets right aligned and another col is added to perform the operation\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size())\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the prediction\n",
        "      logits, loss = self(idx)\n",
        "      # Focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # Apply softmax to get probabilities\n",
        "      # Introduce a small value (epsilon) to add to the logits before softmax\n",
        "      epsilon = 1e-6\n",
        "      probs = F.softmax(logits + epsilon, dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # sample from the distribution\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "      # Applied sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "# _____________________________________________________________________________\n",
        "\n",
        "# Create model\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# Create pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  xb, yb = get_batch('train')\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# Generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FXP0cIUAPQLK",
        "outputId": "7dc3f1a9-9f4e-4bb2-f53a-8a780f446b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5478, val loss 4.5503\n",
            "step 100: train loss 3.5436, val loss 3.5492\n",
            "step 200: train loss 3.0673, val loss 3.0798\n",
            "step 300: train loss 2.8837, val loss 2.9120\n",
            "step 400: train loss 2.7939, val loss 2.8095\n",
            "step 500: train loss 2.7232, val loss 2.7400\n",
            "step 600: train loss 2.6854, val loss 2.6863\n",
            "step 700: train loss 2.6521, val loss 2.6619\n",
            "step 800: train loss 2.6289, val loss 2.6468\n",
            "step 900: train loss 2.6177, val loss 2.6160\n",
            "step 1000: train loss 2.5881, val loss 2.6035\n",
            "step 1100: train loss 2.5865, val loss 2.5923\n",
            "step 1200: train loss 2.5792, val loss 2.5805\n",
            "step 1300: train loss 2.5663, val loss 2.5784\n",
            "step 1400: train loss 2.5477, val loss 2.5668\n",
            "step 1500: train loss 2.5484, val loss 2.5535\n",
            "step 1600: train loss 2.5387, val loss 2.5541\n",
            "step 1700: train loss 2.5345, val loss 2.5466\n",
            "step 1800: train loss 2.5281, val loss 2.5423\n",
            "step 1900: train loss 2.5203, val loss 2.5392\n",
            "step 2000: train loss 2.5225, val loss 2.5354\n",
            "step 2100: train loss 2.5226, val loss 2.5315\n",
            "step 2200: train loss 2.5075, val loss 2.5341\n",
            "step 2300: train loss 2.5145, val loss 2.5268\n",
            "step 2400: train loss 2.5070, val loss 2.5255\n",
            "step 2500: train loss 2.5019, val loss 2.5207\n",
            "step 2600: train loss 2.5023, val loss 2.5175\n",
            "step 2700: train loss 2.5100, val loss 2.5176\n",
            "step 2800: train loss 2.5039, val loss 2.5189\n",
            "step 2900: train loss 2.4969, val loss 2.5115\n",
            "step 3000: train loss 2.4893, val loss 2.5150\n",
            "step 3100: train loss 2.4971, val loss 2.5096\n",
            "step 3200: train loss 2.4972, val loss 2.5036\n",
            "step 3300: train loss 2.4951, val loss 2.5089\n",
            "step 3400: train loss 2.5002, val loss 2.5048\n",
            "step 3500: train loss 2.4845, val loss 2.5026\n",
            "step 3600: train loss 2.4902, val loss 2.5025\n",
            "step 3700: train loss 2.4889, val loss 2.5009\n",
            "step 3800: train loss 2.4922, val loss 2.5127\n",
            "step 3900: train loss 2.4940, val loss 2.5084\n",
            "step 4000: train loss 2.4922, val loss 2.5059\n",
            "step 4100: train loss 2.4873, val loss 2.5043\n",
            "step 4200: train loss 2.4897, val loss 2.5058\n",
            "step 4300: train loss 2.4856, val loss 2.5023\n",
            "step 4400: train loss 2.4914, val loss 2.4976\n",
            "step 4500: train loss 2.4799, val loss 2.5065\n",
            "step 4600: train loss 2.4875, val loss 2.5040\n",
            "step 4700: train loss 2.4816, val loss 2.5023\n",
            "step 4800: train loss 2.4809, val loss 2.5031\n",
            "step 4900: train loss 2.4777, val loss 2.4949\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fb1ab102701d>\u001b[0m in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# Generate from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-fb1ab102701d>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m       \u001b[0midx_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m       \u001b[0;31m# sample from the distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention Logic"
      ],
      "metadata": {
        "id": "3wGj0-IecisK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are implementing self attention for a single individual head\n",
        "using: B, T, C = 4, 8, 32\n",
        "\n",
        "we have a 4x8 arrangement of tokens and info on each token is 32 dimensional.\n",
        "\n",
        "The code we had before did a simple average of all the past tokens and the current token. So the previous info and the current info is being mixed together in an average. (We are masking out the wei matrix)\n",
        "\n",
        "Each number in the previous matrices we developed represented the affinities of each token with one another, which we had uniformly initialized.\n",
        "\n",
        "- Now we don't want to initialize the affinities to be uniform as different tokens would find other tokens more or less interesting\n",
        "\n",
        "- So the problem that self attention solves is that it allows us to gather info from the past in a data dependant way (Need More explanation on this)\n",
        "\n",
        "- The way self attention solves this:\n",
        "  - every single token/node at each position produces 2 vectors: Query(Q) and Key(K)\n",
        "    - Query: What am I looking for?\n",
        "    - Key: What do I contain?\n",
        "  - To get the affinities between these sequenced tokens, we do: Q.K = wei\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnfpxOj8dOpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = nn.Linear(14, 16, bias=False)\n",
        "print((key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2zOadZj2UCt",
        "outputId": "a7daa13e-4a97-4240-e0f1-32b7942a77fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=14, out_features=16, bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# Creating a single head for self attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False) # bias false just makes it apply simple matrix multiplication with it's weights\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)   # (B, T, 16) 16 is the head size\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# The reason for doing the triangular matrix is that if I'm from the 6th node/token, I don't want any information about the 7th, 8th .... nodes, coz they're in the future, I'm\n",
        "# learning from the present and past. The upper triangular matrix does that for us\n",
        "\n",
        "# wei = F.softmax(wei, dim=-1)\n",
        "# Softmax helps us to exponentiate\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXAXeucyLaog",
        "outputId": "18e996b1-036c-49d9-d5af-4e7155b90384"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0] # previously every single batch element had uniform wei, but now, every batch element is different, coz every batch element contains different tokens"
      ],
      "metadata": {
        "id": "fA-YM5DHMjmA",
        "outputId": "74f6b3ae-d793-46f3-da8f-e5bef936ea08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
              "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
              "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
              "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing self attention to the previous ngram model"
      ],
      "metadata": {
        "id": "h9aWiRDvjS9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel\n",
        "block_size = 32 # maximum context length for predictions\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-4  # Consider decreasing further for better convergence if overfitting\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(137)\n",
        "\n",
        "# Load input data\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create a list of unique characters in the text and the size of the vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create mappings from characters to integers and vice versa\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]  # Encodes a string as a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Decodes a list of integers to a string\n",
        "\n",
        "# Encode the entire text dataset and split into train and validation sets\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Function to generate a batch of data\n",
        "def get_batch(split):\n",
        "    # Choose train or validation data based on the split\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # Randomly pick 'batch_size' starting points for the sequences\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # Move data to the GPU (if available)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Function to estimate the loss on the train and validation sets\n",
        "@torch.no_grad()  # Disables gradient calculation\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()  # Return to training mode\n",
        "    return out\n",
        "\n",
        "# One head of self-attention mechanism\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # Triangular matrix to ensure that each token can only attend to previous ones\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        # The above line uses scaled-self attention\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Mask out future tokens\n",
        "        wei = F.softmax(wei, dim=-1)  # Normalize attention scores to probabilities\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "# The bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Embeddings for tokens and positions\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # Single self-attention head\n",
        "        self.sa_head = Head(n_embd)\n",
        "        # Linear layer to produce logits\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # Look up token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb  # Combine token and position embeddings (B, T, C)\n",
        "        x = self.sa_head(x)  # Apply one head of self-attention\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens\n",
        "\n",
        "            # NOTE: Here, we can never have more than block size tokens coming in.\n",
        "            # Else it would run our positional embedding table out of scope\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get predictions\n",
        "            logits, loss = self(idx_cond)  # Use cropped idx\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # Append sampled index to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Create model\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "# Create pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    # Evaluate the model at regular intervals\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # Get a batch of training data\n",
        "    xb, yb = get_batch('train')\n",
        "    # Evaluate the loss\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
        "    loss.backward()  # Backpropagate\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "# Generate text from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Initial context\n",
        "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))  # Generate and decode text\n"
      ],
      "metadata": {
        "id": "1TvdSA90MleJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e85064-7ea5-4b02-cde8-174d5dcf2b50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1989, val loss 4.2035\n",
            "step 500: train loss 3.2869, val loss 3.3216\n",
            "step 1000: train loss 3.1393, val loss 3.1668\n",
            "step 1500: train loss 3.0317, val loss 3.0616\n",
            "step 2000: train loss 2.9532, val loss 2.9741\n",
            "step 2500: train loss 2.8993, val loss 2.9163\n",
            "step 3000: train loss 2.8637, val loss 2.8783\n",
            "step 3500: train loss 2.8285, val loss 2.8432\n",
            "step 4000: train loss 2.8030, val loss 2.8083\n",
            "step 4500: train loss 2.7671, val loss 2.7855\n",
            "\n",
            "GTFl sre iegent\n",
            "CO IAS\n",
            "RL\n",
            "Bofe' sn ke s LD:\n",
            "GO:el lth? tradth;'f the demyoy tusr:foy wy hy I wiiriiror, Etimas fthe, p this t vavheud amonethiuyisabezounl bu enasenghe w my tb isre, h ae rt t h.\n",
            "Adenn assevaulome yifeme encolo se.\n",
            "Tinsy\n",
            "ARqrchanol, flsy bounnd,o;p han\n",
            "Ae ne?iwiln Q fkuML\n",
            "RItawhf usdou hedt ke krfan ruad': le e\n",
            "Afthe teyuned, hipeow. snn Byo Ethawie ngouou was thime fifaan\n",
            "Smeu'iom tfrn seen\n",
            "Miwil er ine tedirapsalsen kundgee as mad t?\n",
            "O:gr arl em,ounrswill ictan:\n",
            "I gh we:pl a I-H ut ng yind othan naceden b scat thhouo'torore, cis b amaoe oy , riws nd s\n",
            "Aeay t oor eicr p f diar th he;,\n",
            "Masenonomhe me mieand t h: Xtoros f giitocinteso y ry e scsif t d mn. lmy wrrt y p;yiranareiro,\n",
            "N, bare, m hilme Wo y yigos Yu e h\n",
            "\n",
            "Hh tet m;:as, fav'tof the I\n",
            ":\n",
            "Woromhon r ace wnd OAoRAmej-;;h ale, ne f\n",
            "OKE, s a :\n",
            "Ty, oiy  ane id,pnfipag-er b, lam;\n",
            "C\n",
            "C\n",
            "F. ho;ihe wi.UD&bdoiul athif s,\n",
            "\n",
            "H?IO\n",
            "Widowy;emasor himl big wy Imnres yo cr e brde.\n",
            "Y heuae we y\n",
            "Awol, am I\n",
            "I, tiqlrme N\n",
            "Oy,\n",
            "YET hoviiae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now heading to Multi Headed Attention\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "THis just means executing multiple attention heads in parallel and concatenating their results\n"
      ],
      "metadata": {
        "id": "6bAC41nXjX9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Attention (Head and MultiHeadAttention Classes):\n",
        "\n",
        "- Self-Attention: The Head class implements one \"head\" of self-attention, where each token (character) in the sequence learns to \"pay attention\" to other tokens before it.\n",
        "- Key, Query, Value: Each token generates three vectors:\n",
        "  - Key: What this token is about.\n",
        "  - Query: What this token is looking for.\n",
        "  - Value: Information passed to other tokens.\n",
        "- Attention scores are computed using the Query and Key vectors, and they determine how much each token should focus on other tokens.\n",
        "- The masked_fill function ensures that the model only looks at previous tokens, not future ones.\n",
        "- Multi-Head Attention: Instead of one attention head, MultiHeadAttention creates several heads (4 in this case). This allows the model to learn different kinds of relationships between characters simultaneously."
      ],
      "metadata": {
        "id": "un88yda4M7r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel\n",
        "block_size = 64 # maximum context length for predictions\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3  # Consider decreasing further for better convergence if overfitting\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 400\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(17)\n",
        "\n",
        "# Load input data\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create a list of unique characters in the text and the size of the vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create mappings from characters to integers and vice versa\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]  # Encodes a string as a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Decodes a list of integers to a string\n",
        "\n",
        "# Encode the entire text dataset and split into train and validation sets\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Function to generate a batch of data\n",
        "def get_batch(split):\n",
        "    # Choose train or validation data based on the split\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # Randomly pick 'batch_size' starting points for the sequences\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # Move data to the GPU (if available)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Function to estimate the loss on the train and validation sets\n",
        "@torch.no_grad()  # Disables gradient calculation\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()  # Return to training mode\n",
        "    return out\n",
        "\n",
        "# One head of self-attention mechanism\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # Triangular matrix to ensure that each token can only attend to previous ones\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Mask out future tokens\n",
        "        wei = F.softmax(wei, dim=-1)  # Normalize attention scores to probabilities\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple heads of self attention in parallel\"\"\"\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1) #concatenating all the heads over the channel dimension\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\"Simple Linear Layer followed by non-linearity\"\"\"\n",
        "  # This computation is to be done on a per-node level, all tokens do this individualy,\n",
        "  # Done once the node has gathered all the data, now they need to think on that data individually\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "# The bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Embeddings for tokens and positions\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # Single self-attention head\n",
        "        # self.sa_head = Head(n_embd)\n",
        "\n",
        "        # Using multiHeaded Attention\n",
        "        self.sa_head = MultiHeadAttention(4, n_embd//4) # i.e 4 heads of 8-dimensional self-attention\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        # Linear layer to produce logits\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # Look up token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "\n",
        "        x = tok_emb + pos_emb  # Combine token and position embeddings (B, T, C)\n",
        "        x = self.sa_head(x)  # Apply one head of self-attention\n",
        "        x = self.ffwd(x) #adding the feed forward\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)  # Use cropped idx\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # Append sampled index to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Create model\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "# Create pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    # Evaluate the model at regular intervals\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # Get a batch of training data\n",
        "    xb, yb = get_batch('train')\n",
        "    # Evaluate the loss\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
        "    loss.backward()  # Backpropagate\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "# Generate text from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Initial context\n",
        "print(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))  # Generate and decode text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtI88qVU__iA",
        "outputId": "da0b3031-6960-4a16-9b70-fe43da7b99ee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1981, val loss 4.1998\n",
            "step 500: train loss 2.6304, val loss 2.6317\n",
            "step 1000: train loss 2.5364, val loss 2.5344\n",
            "step 1500: train loss 2.4902, val loss 2.4869\n",
            "step 2000: train loss 2.4565, val loss 2.4587\n",
            "step 2500: train loss 2.4176, val loss 2.4272\n",
            "step 3000: train loss 2.3659, val loss 2.3826\n",
            "step 3500: train loss 2.3279, val loss 2.3394\n",
            "step 4000: train loss 2.2943, val loss 2.3064\n",
            "step 4500: train loss 2.2611, val loss 2.2800\n",
            "step 5000: train loss 2.2246, val loss 2.2533\n",
            "step 5500: train loss 2.1920, val loss 2.2230\n",
            "step 6000: train loss 2.1627, val loss 2.2053\n",
            "step 6500: train loss 2.1381, val loss 2.1892\n",
            "step 7000: train loss 2.1243, val loss 2.1770\n",
            "step 7500: train loss 2.1080, val loss 2.1630\n",
            "step 8000: train loss 2.0962, val loss 2.1548\n",
            "step 8500: train loss 2.0830, val loss 2.1453\n",
            "step 9000: train loss 2.0740, val loss 2.1394\n",
            "step 9500: train loss 2.0676, val loss 2.1342\n",
            "\n",
            "DUKE VINCE:\n",
            "Sind nas prop, 'ngod pord onecess, but my fis det seo?\n",
            "Risien.\n",
            "\n",
            "KINCEWTo forn.\n",
            "\n",
            "BRILINGBELENTIO:\n",
            "Thesty a hin withat poanced call, me Balked,\n",
            "Ifor tor, I an\n",
            "Thanter bip god; heesesring awarlor\n",
            "And bedcouleden.\n",
            "And? But fanglyedst.\n",
            "Bectlece breew sheraver fors.\n",
            "\n",
            "AUSHENY XRICHET:\n",
            "II pousich a that of mands, loanjus,\n",
            "Fo\n",
            "ICl:\n",
            "Read\n",
            "Haver fither:\n",
            "To linceenthy, say he farpryou to woker had wour hate ty pletly, notwhy kim.\n",
            "Fastorny's liseapor hinde Larfus, youch der ofthe:\n",
            "Feirack' over saour, here que, migler for Cat,\n",
            "Thy buthiasing ane havis?\n",
            "\n",
            "CAGERDWIA:\n",
            "Aurpor. Thou you ou to thuingesssuke son the faut,\n",
            "Fer\n",
            "tall tit make\n",
            "Tl he and\n",
            "Ty we ante, anjont swer thu's thus hom for broce, my for re das,\n",
            "I ove momays shes petears poong a and gren.\n",
            "Fescy thoserim son.\n",
            "Whis You the no thavesess tharde might hou I the das dobest dalians\n",
            "Shersal, I\n",
            "buth thee angood in giugst smes, maond wothat, do warl, is, low'd his beire winghsour I am, nevis nour cous sandeme your owis?\n",
            "\n",
            "And It ordeste Yow;\n",
            "Ging donted higidh hatichats, thas\n",
            "To wirdell.\n",
            "\n",
            "That, She eeas of widonther: ford diurke tono brow!\n",
            "Bourd joth hime: hou youller scome\n",
            "Spout de yourd.\n",
            "\n",
            "Mut.Why whill with. upen? titur as the then searh,\n",
            "Thinke iur my tomentetul winded a wet, or fan shall acksay and,--nul-lay kn yoouws cours elize sixteires shy! nounnber it wir I\n",
            "O: we I ake, the for, wigea; he des\n",
            "Turl seair crias that,\n",
            "An'N ofaing that gooo-pan\n",
            "ULANPeakne bracke ucance.\n",
            "\n",
            "HICOMINIO:\n",
            "Thichwithiord dous well des this-fager' than, stee,\n",
            "bled stoblivoud?-\n",
            "BEDWANTEO:\n",
            "Yonehe on.\n",
            "\n",
            "JUKE PEDWANT:\n",
            "Corer chy a I lis to bath tor no\n",
            "Yelfer mlived fucuer. I to buste show arto sur fit ind thoulf your ar.\n",
            "\n",
            "RALIN BORK:\n",
            "Sgeer, dontewace, than ppler? it I ind sict kiche yours,\n",
            "And thom on not alempooown,-\n",
            "So, to doul tres his my prot's ancen winghm blow as of monote;\n",
            "-ILO:\n",
            "Te saking, scon. Cue the saw sret theer mod\n",
            "He?\n",
            "\n",
            "OTIA:\n",
            "Ank Cacecer make, your is con, of I ling mestick, I wint why theer frone, I of I'th? tithe\n",
            "Sirth; a I YAGABEY ETENG:\n",
            "PAl he well aver.\n",
            "Tiurene? BO daod a wey in chom dol\n",
            "Werecandy Jul prow in of be\n",
            "VA us Lord fir be cato off butat bestikinge\n",
            "Bute empactis waystarm;\n",
            "Puren par!'ll an\n",
            "Bode the trarm coursuke man struent hond buchert?\n",
            "What, not fancay coml a earguid\n",
            "GLon bose is ther\n",
            "Ther:\n",
            "This adell of your meacties srothret do sould never, now the then.\n",
            "LERTUCIO:\n",
            "SI sillet he as hat\n",
            "btay!ne, grem sels, all to he useeat wrom.\n",
            "\n",
            "LOREMCIA:\n",
            "SE the ealt: fer lose, fargearnucte ring; goge?\n",
            "\n",
            "WAGLREY:\n",
            "Wehe soull, I aman that flor mante veers tis conn on hand:\n",
            "Thit the pure ous moordugt;\n",
            "Imigh pay.\n",
            "\n",
            "RORLOFF IHABE:\n",
            "O thatidy whegy fo, is tlone!\n",
            "\n",
            "Bis ciom iden;\n",
            "Ond\n",
            "ALK:\n",
            "PE thous AD O cometes aky, as: che solds hom day.\n",
            "INIUT:\n",
            "No sholve; Min oaw ome itre thart\n",
            "Your celll thon:\n",
            "Aln a therpy sord be or comer a to brot, the hat wors'd noced a hisake the hat on that knece of ind no,\n",
            "Thar, ther thald thow.\n",
            "The wought be me anglow''s ame meeselforsth sweir!\n",
            "\n",
            "KINCI ape degnor, dier.\n",
            "\n",
            "BORGAMIO:\n",
            "I no\n",
            "I some, whour yor 'thating nure he of apbress dacke dill suke tureayd is porsungitio-m heemf a and put tiies,\n",
            "Ond god ashar, all thianill, yeld\n",
            "BA, bris heeves\n",
            "Fas I his art so ping rups sake hinsce, some may\n",
            "I us a the hat in loonrens youlf woucch, wit my and: I tell---dly sof en the tall mord nives py whear!\n",
            "Berl neat hevou vederm to the seartizenots my hit ourso of toor lorss,\n",
            "Wet bee your sicke mack whes russes\n",
            "Wknet to!\n",
            "TEt; tirs an hiceis,-es, eas the in prored do to,\n",
            "And aan, harm'm sand in'd\n",
            "ry, come, I con I deweir, 'p muctende the mathat our ior,\n",
            "I heravou himent as, man angod to hat befaill amped\n",
            "Therte son dion ot tallor its, mort of we rell ond.\n",
            "Fargul tray, winssere dour pecall coringidy ad youdie replotne is lak unes peefewick me, bring jukitestevrenrow Thime to a bearm-ger: a That astet crobecess, worce:\n",
            "Have cell your god\n",
            "Wh manth gote;\n",
            "Andcior, brom nowshich Shat?\n",
            "\n",
            "MER RAREO:\n",
            "Mece'pue jon it aly tour him he asend prisk swok is and to ing: their han o lode tand lorrad to she king and you nowigron use\n",
            "reake yous that mad feom to stork! his mreys wehard satin me sham condk wovereir chat angagesinte, ishis aguchan,\n",
            "Whangs\n",
            "Ond be', he that therress pece's is sweer loce! yord por gage the to rur'dn scuriese da thid he fon his memperfow't lor thome ty ou ou of.\n",
            "3 I O:\n",
            "I prives, near,-ter shalt.\n",
            "And, a and thich cuysilte ou my he dinke beer nou.\n",
            "\n",
            "KING RICHAY:\n",
            "OTAGY HABERIA:\n",
            "\n",
            "FOMKE RARD HAPA:\n",
            "Lerl with's is sur\n",
            "Thatead, I beaved hou short lait't I you bre tor thy\n",
            "Ford:\n",
            "UWou shis fold. Wlor man for busblo dong-\n",
            "Ther hid in sweeiressire worsirentettly!\n",
            "Onduly that youte sends, met thold' thas! gris in bank hat.\n",
            "\n",
            "Yor so. Cains hard of wey me Tlomes nothene hat bouter ben trouns mold the laen the mokn he gre coour I your old deve\n",
            "A kn har srow vinte acay\n",
            "Ton!\n",
            "\n",
            "KE ORK:\n",
            "Son id nouclasw yur mande itodas twartoorve tepe.\n",
            "O:\n",
            "Whir the prins your surear he witer ou hour,\n",
            "Had Whis, bronren rine, hand your God her let\n",
            "Thy ginsee thiok durtoor\n",
            "Bhours himsense brode han i'to sere courmy hank, I he is dour wangend tetcardo-fued the is marestingive uspearces\n",
            "To llisw. Wanes cooin?\n",
            "\n",
            "Ned a and hat' Pre'r man hyould re'd:\n",
            "Shat tis dictrupcoulroverst, so ands\n",
            "That\n",
            "Buranch ard, Clow kar mad! ange this! your paciurme somenturtine id heer:\n",
            "This sund all:\n",
            "Amare he her.\n",
            "Had a sam hen he\n",
            "Ousend doateniness forciold nemaste thom yed yeares surky bus thouninge wanwen farve jone.\n",
            "I 'Tkn spiver fospon to a God?\n",
            "Nold he ascrauncede wor he whater, shave if is earangesstell not I the loweh ple dawnethior evet pus ont be grow swold mores ou wrenly tre,\n",
            "To rand; fay as fleaint whois'steme an coon, of trunge be; not a in gore thichy,\n",
            "Winte,'d daver, treven ar at the now you le fiteer toor, bley upity whad mus.\n",
            "\n",
            "stemirest a toun Co che nos your\n",
            "To hill oomf deme ou the, to asteake I; dauntt, yo nus:\n",
            "What wheis uctrgich thar;\n",
            "tausirk cour, tinker falst. To hen thope ittord cuch\n",
            "O moser\n",
            "Tish tice godhat now whim thiasess, your ardold obarovician, yowrountet is thery\n",
            "y you he wour!\n",
            "Cor:\n",
            "Gidains, loke whe quith dey iur tham'dfor in now bure thar tar on an wal le Musiceme:\n",
            "Anwnourhobe\n",
            "And love lort a the he is thas aand nothe lin, to wioursst, id und?\n",
            "So in I what led, thy his of puts wild, in ang ongood troulor:\n",
            "Thatpedse manindo fus notat, hat he ringend the Tharday he all man, I so withe my ear set, is a arke linke,\n",
            "And our at Ofers pose alifay thin sehand ofe\n",
            "Thate drut a conk'd your, fat darn tard, hin abre!\n",
            "\n",
            "FLUS:\n",
            "OYes\n",
            "Hater\n",
            "Thad:\n",
            "Clecion, it the, his onow' and yeare did noouk,\n",
            "Bint, and low a yous be tels pops, speeaul cay,\n",
            "Douhis lifur thard! I le dhe agaks:\n",
            "Whis anfor stalesepicker broide\n",
            "the binging wal\n",
            "s he the to so, tou trich, my for and owning woter:\n",
            "God thurt\n",
            "\n",
            "Qr:\n",
            "A ONEnted prold of weelf be!\n",
            "DUDATTER:\n",
            "ORou Fauen cow.\n",
            "\n",
            "Whomy, croull tremanted up senler a thou 'tidale hicheran my he tifens; arou thy.\n",
            "\n",
            "Fightlok the hour shate thet neake the prok you thatbly he winced ind.\n",
            "\n",
            "DUTHEMAND:\n",
            "Mock ald ap your, hardser murins our to strownerce beate youp ditoge shall ke dis the hise nell andaull linsoul shoougly wenthy, he sorselave neer this ap the sonack'dde.\n",
            "Nure\n",
            "My tor is your frall:\n",
            "Whand?\n",
            "\n",
            "MOMWARD:\n",
            "Burtul all, so of itwot, your nay that me to thes say, bus icke vicher,\n",
            "And Come your den dardgrys of criere ming a arend.\n",
            "\n",
            "LAGELLA:\n",
            "Douke; whin: a tom this Fonch love heard\n",
            "tent, doopon that you tingiars Lourem, duaplote ret that the 't begech, sangat tit niue!\n",
            "\n",
            "MILETENCE:\n",
            "TRasare, prostid saur\n",
            "No hat dus, he or your dbmelsowicfont to tappl calll.\n",
            "\n",
            "Cinher a not I seeet cour ceargws:\n",
            "Hefart dide in ansbut\n",
            "Rexe brand I a'\n",
            "Al\n",
            "WERSELARO:\n",
            "Tures urere and o's buter:\n",
            "Thicis aseyselside agar!\n",
            "RALY HI:\n",
            "\n",
            "Whirs, steer ton you, and mee ton her bre my nay pron, he tou to avis is mabenaciorgy tor Brow?\n",
            "\n",
            "BASAguncers, bet\n",
            "whis int my man sir the, a and fawy, thet shickss allid: it hig adusing dicch I am man,\n",
            "our ofsak:\n",
            "To sly as thee thim to lecqueccee son vime wouly\n",
            "Horve hads in on ezen lescherjit.\n",
            "\n",
            "Clackeres cof pal prispe.\n",
            "\n",
            "SAPRGABET:\n",
            "Thant thou in thim with bewe brow are,\n",
            "GLOLBRY:\n",
            "So whobleadwer I neat the\n",
            "Fare gy juugh the he shay'nd we fir here lomach hat, per iver,\n",
            "Homersshoulour?\n",
            "\n",
            "Nou Cominge\n",
            "The thou ke ding, hou on lor' cond Suft is't his ben rer, toonefirt ond hen withis law thou ll tou din hearth oft mo,\n",
            "I? dunks melful is id azene thor hapinend us to ard, it To your feell yous?\n",
            "INGAUS:\n",
            "Whe lee fakes seare his sagegee thal he to to I ack!\n",
            "\n",
            "A, than. Butebresteresbly lord the anwen tond wargelfor your hatt wa$l wardy to loveraths\n",
            "Ins. I' milsiuenok then my cooulfovery hatives,\n",
            "Thinger short man me dredis thave\n",
            "Iw's and umpan! Bonay:\n",
            "Ye Lake. I Mouss Cin pay, iner' of our,\n",
            "Ll dhank\n",
            "Whatal hat whavinge fas flem.\n",
            "\n",
            "COLED:\n",
            "Mus, a what he nowet or hing reven obe fonfor hans:\n",
            "An thating annis! Roud will wigh, to he thow theer hatards busty,\n",
            "CEt\n",
            "INGOMI:\n",
            "Nobest leppan boing; on whold kame tene inest your feals me,\n",
            "And uprome slo wewillong 'morer with thangiv my fore witking thand the sis by our lomive hatlad----\n",
            "ASHANTIUS:\n",
            "Ka wal\n",
            "\n",
            "ONGAMPSENA:\n",
            "Trou sceet\n",
            "Woure ity soue mearde.\n",
            "Nond:\n",
            "Good he tho fon.\n",
            "\n",
            "IZELBRALE:\n",
            "Hy\n",
            "FOMIO:\n",
            "SA a for on. I your dis ortunty.\n",
            "\n",
            "CAPPRCEY LIGHALY ERESTH:\n",
            "Ginred reermady what thy me chians, hat lan that the pegped the reack I with thoor Heard whill.\n",
            "Wove shasear; me we\n",
            "sherst later his, to, naceniot, thigeae cad pedfainces!\n",
            "\n",
            "Tworkes. Gold Busecemered to himenlos ingul that nace millatte\n",
            "Wort so the of ward I it shouls tand: to\n",
            "But naus prand oo dit umpes:\n",
            "I of dem Wand Den mais weereft: so and a beaw, will welonced gaunks tor.\n",
            "\n",
            "Sed loth his prit, I wers he sengrow weir thid', wir this; anay.\n",
            "To the pou wabysulsbed:\n",
            "Whou moberiche the he of to stry oft it a mure\n",
            "A reer homple\n",
            "And Hase iso bucen\n",
            "On panarein? took thad! daim munce notgementiore\n",
            "Tor a is grilund prop;\n",
            "What thicope\n",
            "UThand! TO dour tens, tho!\n",
            "\n",
            "Roome shards. Wirinte of to bakntot dos gree lo scree\n",
            "Smen, hou, satin housbut\n",
            "Dody rings, him thad, mand's this Rand hut\n",
            "Con? LI Curethy; durber be know.\n",
            "To lee igk mut wis is allor hip tet.\n",
            "\n",
            "Choprie w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further optimization using blocks\n",
        "Now we try to intersperce the communication with the computation\n",
        "\n",
        "**Optimization** using blocks means dividing model computations into modular chunks, each handling both communication (self-attention) and computation (feed-forward).\n",
        "\n",
        "**Intersperse communication with computation** refers to interleaving steps of attention and processing, rather than doing them sequentially. This reduces memory overhead and improves efficiency for large-scale models, especially when training or inferring on large batches or sequences."
      ],
      "metadata": {
        "id": "9M6qwvMAqNxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here , the complexity/depth of the model is now increasing due to the introduction of Blocks, to perform computation+communication together and multiheaded attention.\n",
        "\n",
        "So to overcome such optimization issues, we use residual connections.\n",
        "\n",
        "Changes for residual connections made in the Block class."
      ],
      "metadata": {
        "id": "0gkGTH6jxoSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel\n",
        "block_size = 64 # maximum context length for predictions\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3  # Consider decreasing further for better convergence if overfitting\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 400\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(137)\n",
        "\n",
        "# Load input data\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create a list of unique characters in the text and the size of the vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create mappings from characters to integers and vice versa\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]  # Encodes a string as a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Decodes a list of integers to a string\n",
        "\n",
        "# Encode the entire text dataset and split into train and validation sets\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Function to generate a batch of data\n",
        "def get_batch(split):\n",
        "    # Choose train or validation data based on the split\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # Randomly pick 'batch_size' starting points for the sequences\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # Move data to the GPU (if available)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Function to estimate the loss on the train and validation sets\n",
        "@torch.no_grad()  # Disables gradient calculation\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()  # Return to training mode\n",
        "    return out\n",
        "\n",
        "# One head of self-attention mechanism\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # Triangular matrix to ensure that each token can only attend to previous ones\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Mask out future tokens\n",
        "        wei = F.softmax(wei, dim=-1)  # Normalize attention scores to probabilities\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "# __________ Class for MultiHeaded Attention _____________________\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple heads of self attention in parallel\"\"\"\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1) #concatenating all the heads over the channel dimension\n",
        "    out = self.proj(out) # Projections is just the linear transformation of the outcome of the above layer.\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ________ FeedForward Class ___________________\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\"Simple Linear Layer followed by non-linearity\"\"\"\n",
        "  # This computation is to be done on a per-node level, all tokens do this individualy,\n",
        "  # Done once the node has gathered all the data, now they need to think on that data individually\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd, n_embd), # This is the projection layer going back into the residual pathway\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "# ____________ Block Class ___________________\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size) # For computation\n",
        "    self.ffwd = FeedForward(n_embd) # For communication\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(x)\n",
        "    x = x  + self.ffwd(x)\n",
        "    return x\n",
        "\n",
        "# The bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Embeddings for tokens and positions\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "\n",
        "        )\n",
        "        # Linear layer to produce logits\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # Look up token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "\n",
        "        x = tok_emb + pos_emb  # Combine token and position embeddings (B, T, C)\n",
        "        x = self.blocks(x) #(B, T, C)\n",
        "\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)  # Use cropped idx\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # Append sampled index to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Create model\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "# Create pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    # Evaluate the model at regular intervals\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # Get a batch of training data\n",
        "    xb, yb = get_batch('train')\n",
        "    # Evaluate the loss\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
        "    loss.backward()  # Backpropagate\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "# Generate text from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Initial context\n",
        "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))  # Generate and decode text\n"
      ],
      "metadata": {
        "id": "A6_gMtdtmINs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80651dce-7a1c-4283-db74-965e208ee040"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6526, val loss 4.6710\n",
            "step 500: train loss 2.4538, val loss 2.4695\n",
            "step 1000: train loss 2.2500, val loss 2.2810\n",
            "step 1500: train loss 2.1233, val loss 2.1601\n",
            "step 2000: train loss 2.0306, val loss 2.0929\n",
            "step 2500: train loss 1.9708, val loss 2.0516\n",
            "step 3000: train loss 1.9201, val loss 2.0183\n",
            "step 3500: train loss 1.8805, val loss 1.9915\n",
            "step 4000: train loss 1.8448, val loss 1.9653\n",
            "step 4500: train loss 1.8148, val loss 1.9619\n",
            "step 5000: train loss 1.7921, val loss 1.9441\n",
            "step 5500: train loss 1.7819, val loss 1.9267\n",
            "step 6000: train loss 1.7660, val loss 1.9186\n",
            "step 6500: train loss 1.7473, val loss 1.9059\n",
            "step 7000: train loss 1.7418, val loss 1.8987\n",
            "step 7500: train loss 1.7291, val loss 1.8932\n",
            "step 8000: train loss 1.7217, val loss 1.8850\n",
            "step 8500: train loss 1.7119, val loss 1.8736\n",
            "step 9000: train loss 1.7139, val loss 1.8715\n",
            "step 9500: train loss 1.6982, val loss 1.8763\n",
            "\n",
            "Getllow Edege man bents his be's,\n",
            "The strow our mant to made to\n",
            "and more on that four be grain areign, antic?\n",
            "Thene, by mine to a put amonets unforbe our tould as ame.\n",
            "Let the is purderesst to pradenn'd are us.\n",
            "\n",
            "Thingbe in of Edward earsain.\n",
            "\n",
            "Mess, flay bountran;\n",
            "Then ceon, it unight upquoke what so gratet of hore\n",
            "Or have leterful eithy out, his own sin thou to withore he was\n",
            "Abid and a with which things enreing I make maid haports!-Noury death may to bight rown to name\n",
            "Ev issan:\n",
            "I go we plook,---stworn in not, I nared sat with tray'd's a here,\n",
            "Eve is of own, riss not their heona ear on from a thee,\n",
            "Plases of me ome mistrow hourthorest for world thou trace;\n",
            "Decontry my live wirt out yirtue?\n",
            "\n",
            "CORIOLANUS:\n",
            "No his good yetimes thee horah the my blonour'st fit him:\n",
            "Why he nor accurit come me noch ang, the hath,\n",
            "such honour I was tistrifice and oxplared,\n",
            "Who them he with to of heaving such doth coomp;\n",
            "Busor hims bight than to their of I had shaul the in woe,\n",
            "Am I his in bashing-\n",
            "Erea hoveian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This part adds layer normalization to the Attention Neurons and layers.\n",
        "\n",
        "It is a slight deviation from the original paper of 'Attention is all you Need'\n",
        "\n",
        "In that paper, we apply these normalizations after the processing of the Attention head and Feed Forward classes.\n",
        "\n",
        "Here we would be doing that before, i.e, we normalize our encoding sum (x) before passing it to the Attention head,\n",
        "as well as the FeedForward layer.\n",
        "\n",
        "We would be using 2 LayerNorm Layers here, one for the attention head and one for the FeedForward layer.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel\n",
        "block_size = 64 # maximum context length for predictions\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3  # Consider decreasing further for better convergence if overfitting\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 400\n",
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(137)\n",
        "\n",
        "# Load input data\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create a list of unique characters in the text and the size of the vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create mappings from characters to integers and vice versa\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]  # Encodes a string as a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Decodes a list of integers to a string\n",
        "\n",
        "# Encode the entire text dataset and split into train and validation sets\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Function to generate a batch of data\n",
        "def get_batch(split):\n",
        "    # Choose train or validation data based on the split\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # Randomly pick 'batch_size' starting points for the sequences\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # Move data to the GPU (if available)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Function to estimate the loss on the train and validation sets\n",
        "@torch.no_grad()  # Disables gradient calculation\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()  # Return to training mode\n",
        "    return out\n",
        "\n",
        "# One head of self-attention mechanism\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # Triangular matrix to ensure that each token can only attend to previous ones\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Mask out future tokens\n",
        "        wei = F.softmax(wei, dim=-1)  # Normalize attention scores to probabilities\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "# __________ Class for MultiHeaded Attention _____________________\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple heads of self attention in parallel\"\"\"\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1) #concatenating all the heads over the channel dimension\n",
        "    out = self.proj(out) # Projections is just the linear transformation of the outcome of the above layer.\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ________ FeedForward Class ___________________\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\"Simple Linear Layer followed by non-linearity\"\"\"\n",
        "  # This computation is to be done on a per-node level, all tokens do this individualy,\n",
        "  # Done once the node has gathered all the data, now they need to think on that data individually\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd, n_embd), # This is the projection layer going back into the residual pathway\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "# ____________ Block Class ___________________\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size) # For computation\n",
        "    self.ffwd = FeedForward(n_embd) # For communication\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Changes made here by adding the LayerNorm to both the layers.\n",
        "    # It's like a per-token\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x  + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "# The bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Embeddings for tokens and positions\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "\n",
        "        )\n",
        "        # Linear layer to produce logits\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # Look up token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "\n",
        "        x = tok_emb + pos_emb  # Combine token and position embeddings (B, T, C)\n",
        "        x = self.blocks(x) #(B, T, C)\n",
        "\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)  # Use cropped idx\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # Append sampled index to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Create model\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "# Create pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    # Evaluate the model at regular intervals\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # Get a batch of training data\n",
        "    xb, yb = get_batch('train')\n",
        "    # Evaluate the loss\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
        "    loss.backward()  # Backpropagate\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "# Generate text from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Initial context\n",
        "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))  # Generate and decode text\n"
      ],
      "metadata": {
        "id": "M1oVJqP93Emj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}